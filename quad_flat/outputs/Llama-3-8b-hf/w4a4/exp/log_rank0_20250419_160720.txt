[2025-04-19 16:07:20 root] (args_utils.py 165): INFO Arguments: 
[2025-04-19 16:07:20 root] (args_utils.py 166): INFO {'a_asym': False,
 'a_bits': 4,
 'a_groupsize': -1,
 'act_order': False,
 'add_diag': True,
 'cache_dir': './outputs/.cache',
 'cali_bsz': 4,
 'cali_dataset': 'wikitext2',
 'cali_trans': True,
 'deactive_amp': False,
 'diag_alpha': 0.3,
 'diag_init': 'sq_style',
 'direct_inv': False,
 'distribute_model': False,
 'epochs': 15,
 'exp_dir': './outputs/Llama-3-8b-hf/w4a4/exp',
 'exp_name': 'exp',
 'flat_lr': 0.005,
 'gptq': False,
 'gptq_mse': False,
 'hf_token': None,
 'k_asym': True,
 'k_bits': 4,
 'k_groupsize': 128,
 'lac': True,
 'lm_eval': False,
 'lm_eval_batch_size': 128,
 'lwc': True,
 'matrix_path': None,
 'model': '/data/models/Llama-3-8b-hf',
 'model_name': 'Llama-3-8b-hf',
 'nsamples': 128,
 'output_dir': './outputs',
 'percdamp': 0.01,
 'pod_rank': 64,
 'q_asym': False,
 'q_bits': 16,
 'q_groupsize': -1,
 'quantize': True,
 'reload_matrix': False,
 'resume': False,
 'save_matrix': False,
 'seed': 0,
 'separate_vtrans': False,
 'tasks': ['piqa',
           'hellaswag',
           'arc_easy',
           'arc_challenge',
           'winogrande',
           'lambada_openai'],
 'v_asym': True,
 'v_bits': 4,
 'v_groupsize': 128,
 'w_asym': False,
 'w_bits': 4,
 'w_groupsize': -1,
 'warmup': False}
[2025-04-19 16:07:20 root] (args_utils.py 167): INFO ------------------------------------------------------------
[2025-04-19 16:07:21 root] (model_utils.py 25): INFO ---> Loading /data/models/Llama-3-8b-hf Model with seq_len: 2048
[2025-04-19 16:07:33 root] (main.py 26): INFO Finished loading training data.
[2025-04-19 16:07:38 root] (main.py 30): INFO Finished applying FlatQuant to model.
[2025-04-19 16:07:40 root] (train_utils.py 90): INFO ========= Layer 0 =========
[2025-04-19 16:08:03 root] (train_utils.py 153): INFO layer 0 lwc lac iter 0, lr 0.00494542  time 15.405908s, mse: 0.00059208
[2025-04-19 16:08:18 root] (train_utils.py 153): INFO layer 0 lwc lac iter 1, lr 0.00478408  time 14.986305s, mse: 0.00022818
[2025-04-19 16:08:33 root] (train_utils.py 153): INFO layer 0 lwc lac iter 2, lr 0.00452302  time 15.021532s, mse: 0.00015127
[2025-04-19 16:08:49 root] (train_utils.py 153): INFO layer 0 lwc lac iter 3, lr 0.00417365  time 15.026365s, mse: 0.00013872
[2025-04-19 16:09:04 root] (train_utils.py 153): INFO layer 0 lwc lac iter 4, lr 0.00375125  time 15.052753s, mse: 0.00014044
[2025-04-19 16:09:19 root] (train_utils.py 153): INFO layer 0 lwc lac iter 5, lr 0.00327427  time 15.089325s, mse: 0.00014263
[2025-04-19 16:09:34 root] (train_utils.py 153): INFO layer 0 lwc lac iter 6, lr 0.00276356  time 15.093980s, mse: 0.00014714
[2025-04-19 16:09:49 root] (train_utils.py 153): INFO layer 0 lwc lac iter 7, lr 0.00224144  time 15.098301s, mse: 0.00014713
[2025-04-19 16:10:04 root] (train_utils.py 153): INFO layer 0 lwc lac iter 8, lr 0.00173073  time 15.118740s, mse: 0.00014216
[2025-04-19 16:10:19 root] (train_utils.py 153): INFO layer 0 lwc lac iter 9, lr 0.00125375  time 15.130973s, mse: 0.00014137
[2025-04-19 16:10:34 root] (train_utils.py 153): INFO layer 0 lwc lac iter 10, lr 0.00083135  time 15.108100s, mse: 0.00014031
[2025-04-19 16:10:49 root] (train_utils.py 153): INFO layer 0 lwc lac iter 11, lr 0.00048198  time 15.122835s, mse: 0.00014068
[2025-04-19 16:11:04 root] (train_utils.py 153): INFO layer 0 lwc lac iter 12, lr 0.00022092  time 15.111211s, mse: 0.00013938
[2025-04-19 16:11:20 root] (train_utils.py 153): INFO layer 0 lwc lac iter 13, lr 0.00005958  time 15.124309s, mse: 0.00013862
[2025-04-19 16:11:35 root] (train_utils.py 153): INFO layer 0 lwc lac iter 14, lr 0.00000500  time 15.108001s, mse: 0.00013810
[2025-04-19 16:11:35 root] (train_utils.py 159): INFO saved paramaters at ./outputs/Llama-3-8b-hf/w4a4/exp/flat_parameters.pth
